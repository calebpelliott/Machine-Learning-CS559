{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f920a13",
   "metadata": {},
   "source": [
    "# <center> CS559 Homework#3: Decision Tree and Ensemble Methods</center>\n",
    "## <center> Due: 11/8/2021 Monday at 11:59 PM</center>\n",
    "\n",
    "\n",
    "In this assignment, you are going to implement four classifiers - **decision tree, random forest, adaboost, and gradient boost**. \n",
    "Then check the performance with `sklearn` built-in algorithms.\n",
    "In this work, splitting into train and test sets is not necessary. \n",
    "\n",
    "The provided data has four columns - three features (a, b, and c) and the target (class). Three features are continuous data and the target is a binary, 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f0fa8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ade2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./F21_CS559_HW3_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89c8ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.4202</td>\n",
       "      <td>-4.3507</td>\n",
       "      <td>10.3764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.7044</td>\n",
       "      <td>-4.4601</td>\n",
       "      <td>10.6803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.8075</td>\n",
       "      <td>-4.0894</td>\n",
       "      <td>10.6259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.2771</td>\n",
       "      <td>-4.0349</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.6447</td>\n",
       "      <td>-3.5968</td>\n",
       "      <td>10.2936</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        a       b        c  class\n",
       "0  9.4202 -4.3507  10.3764      1\n",
       "1  9.7044 -4.4601  10.6803      1\n",
       "2  9.8075 -4.0894  10.6259      1\n",
       "3  9.2771 -4.0349  10.1166      1\n",
       "4  9.6447 -3.5968  10.2936      1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0d970",
   "metadata": {},
   "source": [
    "### Question 1: Decisition Tree Classifier\n",
    "- A simple DT implementation (10 pts.)\n",
    "    - to make the problem simple, implement a decision tree with depth of 3 (the root index is 0).\n",
    "    - calculate the gini index for each attribute and pick the best attribute for each node.\n",
    "    - calculate the accuracy using accuracy score. \n",
    "- Classification using DecistionTreeClassifier (5 pts)\n",
    "- Evaluation (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5f0f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(df):\n",
    "    X = df.drop('class', 1)\n",
    "    y = df['class']\n",
    "    for x in df.columns[0:-1]:\n",
    "        #Cut by mean\n",
    "        mean = df[x].mean()\n",
    "        min = df[x].min()\n",
    "        max = df[x].max()\n",
    "        mid = (max + min) /2\n",
    "\n",
    "        df[x] = pd.cut(df[x], bins=[min, mid, max], labels=[0,1])\n",
    "    def calcGini(item):\n",
    "    #Calculate gini\n",
    "        count = Counter(item)\n",
    "        total = len(item)\n",
    "        count_list = [count[0], count[1]]\n",
    "        freqs = [float(x)/total for x in count_list]\n",
    "        gini_scores = [x*(1-x) for x in freqs]\n",
    "        final_score = sum(gini_scores)\n",
    "        return final_score\n",
    "\n",
    "    def getSortedFeatureScores(df):\n",
    "        scores = {}\n",
    "        for feat in df.columns[0:-1]:\n",
    "            L=list(df[feat])\n",
    "            scores[feat] = calcGini(L)\n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    feat_order = []\n",
    "    pred = {}\n",
    "\n",
    "    node = getSortedFeatureScores(df)\n",
    "    feat_order.append(node[0][0])\n",
    "    l_df = df.loc[df[node[0][0]] == 0]\n",
    "    r_df = df.loc[df[node[0][0]] == 1]\n",
    "    #       c\n",
    "    #     0   1\n",
    "    #   a       a\n",
    "    #  0 1     0 1\n",
    "    # b   b    b  b\n",
    "    #0 1 0 1  0 1 0 1\n",
    "    #2 2 2 2  1 1 1 2\n",
    "    #process left\n",
    "    n_df = l_df.drop(node[0][0],1)\n",
    "    node2 = getSortedFeatureScores(n_df)\n",
    "    ll_df = n_df.loc[n_df[node2[0][0]] == 0]\n",
    "    ll_df = ll_df.drop(node2[0][0],1)\n",
    "    rr_df = n_df.loc[n_df[node2[0][0]] == 1]\n",
    "    rr_df = rr_df.drop(node2[0][0],1)\n",
    "    # percentage of b1 -> class 1 vs class 2\n",
    "    total = len(ll_df)\n",
    "    b0 = ll_df[ll_df[node2[1][0]] ==0]\n",
    "    pc1 = len(b0[b0['class'] == 1]) / len(b0)\n",
    "    pc2 = len(b0[b0['class'] == 2]) / len(b0)\n",
    "    if pc1 > pc2:\n",
    "        pred['000'] = 1\n",
    "    else:\n",
    "        pred['000'] = 2\n",
    "\n",
    "    b1 = ll_df[ll_df[node2[1][0]] ==1]\n",
    "    pc1 = len(b1[b1['class'] == 1]) / len(b1)\n",
    "    pc2 = len(b1[b1['class'] == 2]) / len(b1)\n",
    "    if pc1 > pc2:\n",
    "        pred['001'] = 1\n",
    "    else:\n",
    "        pred['001'] = 2\n",
    "\n",
    "    b0 = rr_df[rr_df[node2[1][0]] ==0]\n",
    "    pc1 = len(b0[b0['class'] == 1]) / len(b0)\n",
    "    pc2 = len(b0[b0['class'] == 2]) / len(b0)\n",
    "    if pc1 > pc2:\n",
    "        pred['010'] = 1\n",
    "    else:\n",
    "        pred['010'] = 2\n",
    "\n",
    "    b1 = rr_df[rr_df[node2[1][0]] ==1]\n",
    "    pc1 = len(b1[b1['class'] == 1]) / len(b1)\n",
    "    pc2 = len(b1[b1['class'] == 2]) / len(b1)\n",
    "    if pc1 > pc2:\n",
    "        pred['011'] = 1\n",
    "    else:\n",
    "        pred['011'] = 2\n",
    "\n",
    "\n",
    "    #process right\n",
    "    n_df = r_df.drop(node[0][0],1)\n",
    "    node2 = getSortedFeatureScores(n_df)\n",
    "    ll_df = n_df.loc[n_df[node2[0][0]] == 0]\n",
    "    ll_df = ll_df.drop(node2[0][0],1)\n",
    "    rr_df = n_df.loc[n_df[node2[0][0]] == 1]\n",
    "    rr_df = rr_df.drop(node2[0][0],1)\n",
    "    # percentage of b1 -> class 1 vs class 2\n",
    "    total = len(ll_df)\n",
    "    b0 = ll_df[ll_df[node2[1][0]] ==0]\n",
    "    pc1 = len(b0[b0['class'] == 1]) / len(b0)\n",
    "    pc2 = len(b0[b0['class'] == 2]) / len(b0)\n",
    "    if pc1 > pc2:\n",
    "        pred['100'] = 1\n",
    "    else:\n",
    "        pred['100'] = 2\n",
    "\n",
    "    b1 = ll_df[ll_df[node2[1][0]] ==1]\n",
    "    pc1 = len(b1[b1['class'] == 1]) / len(b1)\n",
    "    pc2 = len(b1[b1['class'] == 2]) / len(b1)\n",
    "    if pc1 > pc2:\n",
    "        pred['101'] = 1\n",
    "    else:\n",
    "        pred['101'] = 2\n",
    "\n",
    "    b0 = rr_df[rr_df[node2[1][0]] ==0]\n",
    "    pc1 = len(b0[b0['class'] == 1]) / len(b0)\n",
    "    pc2 = len(b0[b0['class'] == 2]) / len(b0)\n",
    "    if pc1 > pc2:\n",
    "        pred['110'] = 1\n",
    "    else:\n",
    "        pred['110'] = 2\n",
    "\n",
    "    b1 = rr_df[rr_df[node2[1][0]] ==1]\n",
    "    pc1 = len(b1[b1['class'] == 1]) / len(b1)\n",
    "    pc2 = len(b1[b1['class'] == 2]) / len(b1)\n",
    "    if pc1 > pc2:\n",
    "        pred['111'] = 1\n",
    "    else:\n",
    "        pred['111'] = 2\n",
    "\n",
    "    pred_test = []\n",
    "    #do predictions\n",
    "    for index, row in df.iterrows():\n",
    "        key = str(row['c']) + str(row['a']) + str(row['b'])\n",
    "        try:\n",
    "            pred_test.append(pred[key])\n",
    "        except:\n",
    "            pred_test.append(1)\n",
    "    score = accuracy_score(pred_test, y)\n",
    "    return score\n",
    "selfscore = DecisionTree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51be80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X,y)\n",
    "score = dtc.score(X,y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd62b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement score: 0.972\n",
      "sklearn score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Implement score: {selfscore}\")\n",
    "print(f\"sklearn score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409c260",
   "metadata": {},
   "source": [
    "### Question 2: Random Forest Classifier\n",
    "- A simle RF implementation (10 pts)\n",
    "    - make a bootstrap baggin function to make 3 samples.\n",
    "    - for each sample, run a simple DT from question 1.\n",
    "    - then average the accuracy. \n",
    "- Classification using RandomForestClassifier (5 pts)\n",
    "- Evaluation (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76d14a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "df = pd.read_csv('./F21_CS559_HW3_data.csv')\n",
    "\n",
    "def buildRandomForest(df):\n",
    "    ratio = .5\n",
    "    rand_i = []\n",
    "    needed= round(len(df) * ratio)\n",
    "\n",
    "    for i in range(needed):\n",
    "        rand_i.append(randrange(len(df)))\n",
    "    new_df = df.iloc[rand_i,:]\n",
    "    return new_df\n",
    "\n",
    "all_scores = []\n",
    "for _ in range(3):\n",
    "    rforest = buildRandomForest(df)\n",
    "    r_score = DecisionTree(rforest)\n",
    "    all_scores.append(r_score)\n",
    "selfscore = sum(all_scores) / len(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12b6c1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X,y)\n",
    "score = rfc.score(X,y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ede15d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Implement score: 0.9632\n",
      "sklearn score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Random forest Implement score: {selfscore}\")\n",
    "print(f\"sklearn score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c04de",
   "metadata": {},
   "source": [
    "### Question 3: AdaBoost Classifier\n",
    "- AB implementation (15 pts)\n",
    "- Classification using AdaBoostClassifier (5 pts)\n",
    "- Evaluation (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f7e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "2495    1\n",
      "2496    1\n",
      "2497    1\n",
      "2498    1\n",
      "2499    1\n",
      "Length: 2500, dtype: int32\n",
      "0.4996\n"
     ]
    }
   ],
   "source": [
    "# Compute error rate, alpha and w\n",
    "def compute_error(y, y_pred, w_i):\n",
    "    '''\n",
    "    Calculate the error rate of a weak classifier m. Arguments:\n",
    "    y: actual target value\n",
    "    y_pred: predicted value by weak classifier\n",
    "    w_i: individual weights for each observation\n",
    "    \n",
    "    Note that all arrays should be the same length\n",
    "    '''\n",
    "    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))/sum(w_i)\n",
    "\n",
    "def compute_alpha(error):\n",
    "    '''\n",
    "    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called\n",
    "    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:\n",
    "    error: error rate from weak classifier m\n",
    "    '''\n",
    "    return np.log((1 - error) / error)\n",
    "\n",
    "def update_weights(w_i, alpha, y, y_pred):\n",
    "    ''' \n",
    "    Update individual weights w_i after a boosting iteration. Arguments:\n",
    "    w_i: individual weights for each observation\n",
    "    y: actual target value\n",
    "    y_pred: predicted value by weak classifier  \n",
    "    alpha: weight of weak classifier used to estimate y_pred\n",
    "    '''  \n",
    "    return w_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))\n",
    "\n",
    "\n",
    "# Define AdaBoost class\n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alphas = []\n",
    "        self.G_M = []\n",
    "        self.M = None\n",
    "        self.training_errors = []\n",
    "        self.prediction_errors = []\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using fitted model. Arguments:\n",
    "        X: independent variables - array-like\n",
    "        '''\n",
    "\n",
    "        # Initialise dataframe with weak predictions for each observation\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.M)) \n",
    "\n",
    "        # Predict class label for each weak classifier, weighted by alpha_m\n",
    "        for m in range(self.M):\n",
    "            y_pred_m = self.G_M[m].predict(X) * self.alphas[m]\n",
    "            weak_preds.iloc[:,m] = y_pred_m\n",
    "\n",
    "        # Calculate final predictions\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, M = 100):\n",
    "        '''\n",
    "        Fit model. Arguments:\n",
    "        X: independent variables - array-like matrix\n",
    "        y: target variable - array-like vector\n",
    "        M: number of boosting rounds. Default is 100 - integer\n",
    "        '''\n",
    "        \n",
    "        # Clear before calling\n",
    "        self.alphas = [] \n",
    "        self.training_errors = []\n",
    "        self.M = M\n",
    "\n",
    "        # Iterate over M weak classifiers\n",
    "        for m in range(0, M):\n",
    "            \n",
    "            # Set weights for current boosting iteration\n",
    "            if m == 0:\n",
    "                w_i = np.ones(len(y)) * 1 / len(y)  # At m = 0, weights are all the same and equal to 1 / N\n",
    "            else:\n",
    "                # (d) Update w_i\n",
    "                w_i = update_weights(w_i, alpha_m, y, y_pred)\n",
    "            \n",
    "            # (a) Fit weak classifier and predict labels\n",
    "            G_m = DecisionTreeClassifier(max_depth = 1)     # Stump: Two terminal-node classification tree\n",
    "            G_m.fit(X, y, sample_weight = w_i)\n",
    "            y_pred = G_m.predict(X)\n",
    "            \n",
    "            self.G_M.append(G_m) # Save to list of weak classifiers\n",
    "\n",
    "            # (b) Compute error\n",
    "            error_m = compute_error(y, y_pred, w_i)\n",
    "            self.training_errors.append(error_m)\n",
    "\n",
    "            # (c) Compute alpha\n",
    "            alpha_m = compute_alpha(error_m)\n",
    "            self.alphas.append(alpha_m)\n",
    "\n",
    "        assert len(self.G_M) == len(self.alphas)\n",
    "\n",
    "myabc= AdaBoost()\n",
    "myabc.fit(X,y,500)\n",
    "pred = myabc.predict(X) \n",
    "print(pred)\n",
    "print(accuracy_score(pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X,y)\n",
    "score = abc.score(X,y)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd5831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e102e54",
   "metadata": {},
   "source": [
    "### Question 4: Gradient Boost Classifier\n",
    "- GB implementation (15 pts)\n",
    "- Classification using GradientBoostingClassifier (5 pts)\n",
    "- Evaluation (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6087e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-93db980d604d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mmygb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmygb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#selfscore = accuracy_score(mygb, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-93db980d604d>\u001b[0m in \u001b[0;36mGradBoost\u001b[1;34m(model, X_train, y_train, boosting_rounds, learning_rate, verbose)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboosting_rounds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# fit the model to the pseudo residuals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpseudo_resids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;31m# increment y_hat_train with the predicted resids*lr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0my_hat_train\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \"\"\"\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    181\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    182\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "from sklearn.linear_model import Ridge\n",
    "def GradBoost(model,\n",
    "              X_train: np.array,         # training independent vars\n",
    "              y_train: np.array,         # training dependent var\n",
    "              boosting_rounds: int = 100,# number of boosting rounds\n",
    "              learning_rate: float = 0.1,# learning rate\n",
    "              verbose: bool = True       # shows progress bar\n",
    "              ) -> np.array: \n",
    "    '''\n",
    "    Takes in a model and performs gradient boosting using it.\n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # initalize guess of our training target variable using the mean\n",
    "    y_hat_train = np.repeat(np.mean(y_train), len(y_train))\n",
    "    \n",
    "    # initialize out of sample prediction with training mean\n",
    "    y_hat_train_test = np.repeat(np.mean(y_train), len(X_train))\n",
    "    # calculate the training residuals fusing the first guess\n",
    "    pseudo_resids = y_train - y_hat_train\n",
    "    \n",
    "    # performs gradient boosting with a tqdm progress bar\n",
    "    if verbose:\n",
    "        from tqdm import tqdm\n",
    "        # iterates through the boosting round\n",
    "        for _ in tqdm(range(0, boosting_rounds)):\n",
    "            # fit the model to the pseudo residuals\n",
    "            model = model.fit(X_train, pseudo_resids)\n",
    "            # increment y_hat_train with the predicted resids*lr\n",
    "            y_hat_train += learning_rate * model.predict(X_train)  \n",
    "     \n",
    "            # increment the predicted test y as well\n",
    "            y_hat_train_test += (learning_rate *   \n",
    "                                 model.predict(X_train))\n",
    "            # calculate the pseudo resids for next round\n",
    "            pseudo_resids = y_train - y_hat_train\n",
    "    # performs gradient boosting without a progress bar        \n",
    "    else:\n",
    "        # iterates through the boosting round\n",
    "        for _ in range(0, boosting_rounds):\n",
    "            # fit the model to the pseudo residuals\n",
    "            model = model.fit(X_train, pseudo_resids)\n",
    "            # increment the y_hat_train with the pseudo resids*lr\n",
    "            y_hat_train += learning_rate * model.predict(X_train)\n",
    "       \n",
    "            # increment the predicted test y as well\n",
    "            y_hat_train_test += (learning_rate * \n",
    "                                 model.predict(X_train))\n",
    "            # calculate the pseudo resids for next round\n",
    "            pseudo_resids = y_train - y_hat_train\n",
    "    # return a tuple of the training y_hat and the test y_hat\n",
    "    return model.predict(X_train)\n",
    "\n",
    "df = pd.read_csv('./F21_CS559_HW3_data.csv')\n",
    "X = df.drop('class', 1)\n",
    "y = df['class']\n",
    "mygb = GradBoost(DecisionTreeClassifier(), X,y)\n",
    "print(mygb)\n",
    "#selfscore = accuracy_score(mygb, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ed0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X,y)\n",
    "pred = gbc.predict(X)\n",
    "print(accuracy_score(y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82528423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
